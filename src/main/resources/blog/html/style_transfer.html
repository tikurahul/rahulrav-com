
  <!doctype html>
    <html lang="en">
      <head>
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>Style Transfer from first principles</title>
        <link rel="preconnect" href="https://fonts.gstatic.com">
      </head>
      <body>
        
      <header class="container-fluid">
        <nav>
          <ul>
            <li><strong>Rahul Ravikumar's Blog</strong></li>
          </ul>
          <ul>
            <li><a href="/blog/toc.html" class="contrast">More Articles</a></li>
          </ul>
        </nav>
      </header>
      <main class="container">
        <p>Apr 22 2020, Tue</p>
<h2 id="style-transfer-from-first-principles">Style Transfer from first principles</h2>
<h3 id="preface">Preface</h3>
<p>Ever since I started dabbling with Deep Learning, one of the papers I have wanted to implement from scratch is <a href="https://arxiv.org/abs/1508.06576">A Neural Algorithm for Artistic Style</a>. This paper completely transformed my perception about Deep Learning. This is because, you could see the model do something very magical; taking an ordinary photograph and adding a masters' touch to it.</p>
<h3 id="introduction">Introduction</h3>
<p>For <code class="prettyprint">Style Transfer</code> you need 2 images. The image you want to add some style to is called a <code class="prettyprint">content</code> image. The style comes from a <code class="prettyprint">style</code> image. </p>
<p>The idea behind <code class="prettyprint">Style Transfer</code> is that you have models like <code class="prettyprint">VGG19</code> which have a higher level understanding of what the components of an image are. The deeper layers in these models understand things like <code class="prettyprint">texture</code> in images. </p>
<p>So if we took the activations of these deep layers for both the <code class="prettyprint">content</code> and <code class="prettyprint">style</code> images, and transformed the activations of the <code class="prettyprint">content</code> image such that it started to look more like the <code class="prettyprint">style</code> image, while still maintaining resemblance to the original <code class="prettyprint">content</code> image; we would successfully <code class="prettyprint">inject</code> style into the <code class="prettyprint">content</code> image.</p>
<h3 id="implementation">Implementation</h3>
<h4 id="a-quick-note">A Quick Note</h4>
<p>I am going to use <a href="https://pytorch.org/"> <code class="prettyprint">PyTorch</code></a> to implement this paper. There is an excellent <code class="prettyprint">PyTorch</code> <a href="https://pytorch.org/tutorials/advanced/neural_style_tutorial.html">tutorial</a> on this topic as well which serves as an excellent reference. </p>
<p>I errored on the side of keeping the code snippets easy to read. If I were to implement this model for production, there would be a lot of refactoring.</p>
<p>The full source code in this article is <a href="https://colab.research.google.com/drive/1WK3wimT4ijpBv8c5U4fKcSrvpQlgqr3x">here</a>.</p>
<h4 id="layer-activations">Layer Activations</h4>
<p>To get started with <code class="prettyprint">Style Tranfer</code> it is clear that we need the layer activations from the deeper layers in a model like <code class="prettyprint">VGG19</code>. So lets write a class that can extract the layer activations for a given image. For this we can use the pretrainined models that <code class="prettyprint">PyTorch</code> has. </p>
<p>First, let's get some imports &amp; setup out of the way.</p>
<pre class="prettyprint linenums"><code class="python language-python">import matplotlib.pyplot as plt
from PIL import Image
import torch
import torch.optim as optim
import torch.nn as nn
import torch.nn.functional as F
import torchvision.models as models
import torchvision.transforms as transforms

# Makes the plots bigger
plt.rcParams["figure.figsize"]= (10,10)</code></pre>
<p> <code class="prettyprint">PyTorch</code> does not make intermediate layer activations available out of the box. So we will wrap the <code class="prettyprint">VGG19</code> with our own <code class="prettyprint">nn.Module</code> and extract the activations we care about in the models' <code class="prettyprint">forward</code> pass.</p>
<pre class="prettyprint linenums"><code class="python language-python">class Vgg19(nn.Module):
  def __init__(self, debug=False):
    super(Vgg19, self).__init__()
    self._vgg19 = models.vgg19(pretrained = True).cuda()
    self._vgg19.train(False)
    self.features = list(self._vgg19.features)

    if debug:
      for i, feature in enumerate(self.features):
        print(i, feature)

    for feature in self.features:
      # No gradient descent required for modules, we don't really want to train the model.
      feature.requries_grad=False
      # Registers a forward hook to intercept layer activations and store them.
      feature.register_forward_hook(self._hook)

      self.content_layer_outputs = []
      self.style_layer_outputs = []

      content_modules = [
        # block_4_conv_2
        self.features[21]
      ]

      self.content_modules = set(content_modules)

      style_modules = [
        # block_1_conv_1
        self.features[0],
        # block_2_conv_1
        self.features[5],
        # block_3_conv_1
        self.features[10],
        # block_4_conv_1
        self.features[19],
        # block_5_conv_1
        self.features[28]
      ]
      self.style_modules = set(style_modules)

  def _hook(self, module, input_, output_):
    if module in self.content_modules:
      self.content_layer_outputs.append(output_)  
    elif module in self.style_modules:
      self.style_layer_outputs.append(output_)
    return None

  def clear(self):
    self.content_layer_outputs = []
    self.style_layer_outputs = []

  def forward(self, x):
    return self._vgg19(x)</code></pre>
<p>Here we define a <code class="prettyprint">nn.Module</code> whhere the <code class="prettyprint">forward</code> pass delegates to the underlying <code class="prettyprint">VGG19</code> model. Let's look a few bits in the source code in more detail.</p>
<pre class="prettyprint linenums"><code class="python language-python">self._vgg19 = models.vgg19(pretrained = True).cuda()
self._vgg19.train(False)</code></pre>
<p>In the above lines, we are creating an instance of the pretrained <code class="prettyprint">vgg19</code> model. We want to run the model on the GPU which is why we are calling <code class="prettyprint">.cuda()</code> on the model. The other important bit here is that we are marking the model as not <code class="prettyprint">trainable</code>. This is because we don't really want to train the model here. All we want are the layer activations. </p>
<p>The layers that we are interested in are defined in the sets <code class="prettyprint">content_modules</code> and <code class="prettyprint">style_modules</code>.</p>
<p>We attach a forward <code class="prettyprint">hook</code> to every sub-module in the <code class="prettyprint">vgg19</code> model so we can intercept the forward passes and store the layer activations.</p>
<h4 id="loss-functions">Loss Functions</h4>
<p>We now have a way to extract the layer activations we care about from <code class="prettyprint">vgg19</code>. We also define 2 sets of layers that we are interested in. <code class="prettyprint">content_layer_outputs</code> are going to useful to ensure that the output image has resemblance to the original input image. <code class="prettyprint">style_layer_outputs</code> will be useful to determine if the artistic styles of the output image match the style in the <code class="prettyprint">style</code> image. </p>
<p>Let <code class="prettyprint">I</code> be the original input image. <code class="prettyprint">S</code> the style image. The <code class="prettyprint">nn.Module</code> we defined can be used to extract content &amp; style outputs respectively.<br />
If we wrote some pseudo code on how to extract the layer activations we are interested in, that would look something like:</p>
<pre class="prettyprint linenums"><code class="python language-python">vgg19 = VGG19()
# Forward pass on the input content image
vgg(I)
content_layer_outputs = vgg['content_layer_outputs']

# Clear intermediate state
vgg19.clear()

# Forward pass on the input style image
vgg(S)
style_layer_outputs = vgg['style_layer_outputs']</code></pre>
<p>The next step is to be able to define loss functions. We need to define multiple loss functions here. This is because we have multiple constraints. </p>
<ol>
<li>The <code class="prettyprint">output</code> image should still look like the original input image. ( <code class="prettyprint">feature_loss</code>)</li>
<li>The <code class="prettyprint">output</code> image should have the artistic style of the <code class="prettyprint">style</code> image. ( <code class="prettyprint">style_loss</code>)</li>
</ol>
<p>For <code class="prettyprint">1</code>, we need to define a loss function called <code class="prettyprint">feature_loss</code> which compares the mean squared error between the <code class="prettyprint">content_layer_output</code>s of the <code class="prettyprint">content</code> image, and the <code class="prettyprint">content_layer_output</code>s of the <code class="prettyprint">output</code> image.</p>
<pre class="prettyprint linenums"><code class="python language-python">def feature_loss(input_features, output_features):
  return F.mse_loss(input_features, output_features)</code></pre>
<p> <code class="prettyprint">2</code> is a bit more complicated. To define <code class="prettyprint">style_loss</code>, We have to compare <code class="prettyprint">style_layer_output</code>s of the <code class="prettyprint">style</code> image with the <code class="prettyprint">style_layer_output</code>s of the <code class="prettyprint">output</code> image.</p>
<p> <code class="prettyprint">style_layer_output</code>s in both cases are n-dimensional arrays of shape <code class="prettyprint">(B, C, H, W)</code> where <code class="prettyprint">B</code> is the <code class="prettyprint">batch_size</code>, <code class="prettyprint">C</code> are the number of channels (in our case filter outputs in layer activations), <code class="prettyprint">H</code> is the height and <code class="prettyprint">W</code> is the width of the layer activations.</p>
<pre class="prettyprint linenums"><code class="python language-python">def gram_matrix(tensor):
  a, b, c, d = tensor.size()
  # Create a 2D view of the tensor
  features = tensor.view(a * b, c * d)
  # Compute the gram matrix
  G = torch.mm(features, features.t())
  # Normalize it by the number of features
  return G.div(a * b * c * d)

def style_loss(style_features, output_features):
  b, c, h, w = output_features.shape
  A = gram_matrix(style_features)
  G = gram_matrix(output_features)
  return F.mse_loss(A, G)</code></pre>
<p>To compute <code class="prettyprint">style_loss</code> we need to understand something called a gram matrix.<br />
For a matrix <code class="prettyprint">M</code> with shape ( <code class="prettyprint">R</code>, <code class="prettyprint">C</code>) for rows &amp; columns, the <code class="prettyprint">gram_matrix</code> = <code class="prettyprint">M * M.t()</code> where <code class="prettyprint">M.t()</code> is the <code class="prettyprint">transpose</code> of <code class="prettyprint">M</code>. <br />
The significance of the Gram Matrix is that it gives a set of linearly indipendent colums. </p>
<p>Once we have the <code class="prettyprint">gram_matrix</code> defined, all we need to do is to compute the mean-squared-error loss between the ouputs of the <code class="prettyprint">gram_matricex</code> of the <code class="prettyprint">style_layer_output</code>s of the <code class="prettyprint">style</code> image with the <code class="prettyprint">style_layer_output</code>s of the <code class="prettyprint">output</code> image.</p>
<p>Finally, we can now define the <code class="prettyprint">total_loss</code> function which is just a weighted sum of the above 2 losses.</p>
<pre class="prettyprint linenums"><code class="python language-python">def loss_fn(b_content_features, s_style_features, input_tensor):
  '''
  b_content_features are the content_layer_outputs from the original image
  s_style_features are the style_layer_outputs from the style image
  input_tensor is the tensor we are interested in
  '''

  batch = input_tensor.unsqueeze(0)
  # Compute the content &amp; style layer outputs for the `input_tensor`
  # vgg19 is an instance of the class we defined in the previous step
  vgg19(batch)
  c_features = vgg19.content_layer_outputs
  s_features = vgg19.style_layer_outputs

  # Compute feature losses
  c_loss = 0.
  for i in range(len(c_features)):
    c_loss += feature_loss(b_content_features[i], c_features[i])

  # Compute style losses
  s_loss = 0.
  for i in range(len(s_features)):
    s_loss += style_loss(s_style_features[i], s_features[i])

  # Weighted sum
  total_loss = c_loss + 100_00_00 * s_loss
  return total_loss</code></pre>
<h4 id="the-optimization-loop">The Optimization Loop</h4>
<p>Now that we have the loss functions, we just need to put everything together. Let's load the <code class="prettyprint">content</code> image first.</p>
<pre class="prettyprint linenums"><code class="python language-python">base_image = Image.open('/gdrive/My Drive/colab/style_transfer/labrador.jpg')
plt.imshow(base_image)</code></pre>
<p><img class="card"  src="/assets/images/labrador.png" alt="Content Image" /></p>
<p>Now, lets load the <code class="prettyprint">style</code> image. I am using Kandinsky's 7th composition for the style image. </p>
<pre class="prettyprint linenums"><code class="python language-python">style_image = Image.open('/gdrive/My Drive/colab/style_transfer/7th_composition.jpg')
plt.imshow(style_image)</code></pre>
<p><img class="card"  src="/assets/images/kandinskys_7th.png" alt="Style Image" /></p>
<p>Let's resize both the images so they have the same shape.</p>
<pre class="prettyprint linenums"><code class="python language-python">style_image = style_image.resize((base_image.width, base_image.height))
plt.imshow(style_image)

to_tensor = transforms.ToTensor()
# Normalize the input image. For more information on this, please look at the linked Colab Notebook. 
base_tensor = normalize(to_tensor(base_image).cuda())
print(base_tensor.shape)

style_tensor = normalize(to_tensor(style_image).cuda())
print(style_tensor.shape)</code></pre>
<p>This gives us:</p>
<pre class="prettyprint linenums"><code>torch.Size([3, 577, 700])
torch.Size([3, 577, 700])</code></pre>
<p>Let's do some setup for the optimization loop. </p>
<pre class="prettyprint linenums"><code class="python language-python"># Create a new instance of Vgg19
vgg19 = Vgg19()

# A helper to clone a list of tensors
def copy(t_list):
  return [t.detach() for t in t_list]

# Extract Content Features
vgg19(base_tensor.unsqueeze(0))
b_content_features = copy(vgg19.content_layer_outputs)

vgg19.clear()

# Extract Style Features
vgg19(style_tensor.unsqueeze(0))
s_style_features = copy(vgg19.style_layer_outputs)

vgg19.clear()</code></pre>
<p>Let's create an <code class="prettyprint">output_tensor_</code> which is the same as the <code class="prettyprint">base_image</code> ( <code class="prettyprint">content</code> image) to start with. <br />
Note that we are calling <code class="prettyprint">requires_grad_()</code> on the <code class="prettyprint">output_tensor_</code> because we want <code class="prettyprint">PyTorch</code> to keep track of the gradients (for gradient descent).<br />
We the define a <code class="prettyprint">optimizer</code> with a list of parameters that <em>only</em> includes the <code class="prettyprint">output_tensor_</code>. This is the only parameter that it needs to optimize.</p>
<pre class="prettyprint linenums"><code class="python language-python"># Start with the input image
output_tensor_ = base_tensor.clone().detach()
# Add requires_grad because we want to compute the gradients
output_tensor_ = output_tensor_.requires_grad_()

# Define optimizer
optimizer = optim.LBFGS([output_tensor_])</code></pre>
<p>We are now ready to define the optimization loop.</p>
<pre class="prettyprint linenums"><code class="python language-python">vgg19.clear()
run = [0]

def step_fn():
  # At every step clear the previous gradients
  optimizer.zero_grad()
  # Clamp the output tensor values between 0 - 1
  output_tensor_.data.clamp(0, 1)
  # Compute loss
  loss = loss_fn(b_content_features, s_style_features, output_tensor_)
  # Extract the loss value as a scalar`
  loss_value = loss.item()
  # Compute the gradients
  loss.backward()
  # Clear intermediate state of the nn.Module we defined
  vgg19.clear()

  if run[0] % 50 == 0:
    # Prints the loss every 50 iterations
    print(run[0], loss_value)

  run[0] += 1
  return loss_value

def optimization_loop():
  max_epochs = 350
  while run[0] &lt; max_epochs:
    optimizer.step(step_fn)

  # Clamp the outputs between 0 - 1 for the last step
  output_tensor_.data.clamp(0, 1)
  print('Done')

optimization_loop()</code></pre>
<p>The <code class="prettyprint">optimizer</code> adjusts the <code class="prettyprint">output_tensor_</code> at every <code class="prettyprint">step</code> to minimize the cumulative <code class="prettyprint">loss</code>. </p>
<h4 id="results">Results</h4>
<p>Finally, it's time to look at the results.</p>
<pre class="prettyprint linenums"><code class="python language-python"># We are using .data because we want the values of the output_tensor without affecting the gradients
results = output_tensor_.data

print(results.shape)
# Convert the output_tensor which was normalized back to an image
results = deprocess(results, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
result_image = transforms.ToPILImage()(results)

# Show image
plt.imshow(result_image)</code></pre>
<p><img class="card"  src="/assets/images/labrador_kandiskys_7th.png" alt="Output Image" /></p>
        <section class="footer">
          <p>
          Rahul Ravikumar &nbsp;
            <a rel="me" href="https://github.com/tikurahul">GitHub</a> &nbsp; | &nbsp;
            <a rel="me" href="https://bsky.app/profile/rahulrav.com">Bluesky</a> &nbsp; | &nbsp;
            <a rel="me" href="https://www.linkedin.com/in/rahulrav/">LinkedIn</a> &nbsp; | &nbsp;
            <a rel="me" href="https://androiddev.social/@rahulrav">AndroidDev Mastodon</a> &nbsp; | &nbsp;
            <a rel="me" href="https://rahulrav.svbtle.com/">Svbtle</a> &nbsp; | &nbsp;
            <a rel="me" href="https://twitter.com/tikurahul">Twitter</a> &nbsp;
          </p>
        </section>
      </main>
    </div>
  
        <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Roboto&family=Zilla+Slab&display=swap">
        <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=JetBrains+Mono&display=swap">
        <link rel="stylesheet" href="/assets/highlight/styles/atom-one-dark.min.css">
        <link rel="stylesheet" href="/assets/core/page.css">
        <script type="text/javascript" src="/assets/core/page.js"></script>
        <script type="text/javascript" src="/assets/highlight/highlight.min.js"></script>
        <script type="text/javascript">
          // Highlight code snippets
          document.addEventListener('DOMContentLoaded', (event) => {
            document.querySelectorAll('pre code').forEach((block) => {
              hljs.highlightElement(block);
            });
          });
        </script>
      </body>
    </html>
  