
  <!doctype html>
    <html lang="en">
      <head>
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>Computational Cameras and Depth AI</title>
        <link rel="preconnect" href="https://fonts.gstatic.com">
      </head>
      <body>
        
      <header class="container-fluid">
        <nav>
          <ul>
            <li><strong>Rahul Ravikumar's Blog</strong></li>
          </ul>
          <ul>
            <li><a href="/blog/toc.html" class="contrast">More Articles</a></li>
          </ul>
        </nav>
      </header>
      <main class="container">
        <p>Jan 17 2021, Sunday</p>
<h2 id="computational-cameras-and-depthai">Computational Cameras and DepthAI</h2>
<p>Modern phones have amazing computational cameras. As a developer, you have access to the RAW image stream and you can post-process frames harnessing the full power of the GPU; or better yet, more dedicated hardware.</p>
<p>My first stand-alone computational camera was a <a href="https://en.wikipedia.org/wiki/Samsung_Galaxy_Camera">Samsung Galaxy Camera</a>. This ran the Android which meant, you had access to all the Camera APIs. This was great because one could use familiar tools and deploy apps that could run on the camera itself. </p>
<p>Another project that comes to mind is <a href="http://jevois.org/">JeVois</a>. This camera was more interesting, because you had access to the full suite of OpenCV APIs which could run on the camera itself (accelerated by the onboard GPU).</p>
<p>Recently I have been experimenting with <a href="https://luxonis.com/depthai">DepthAI</a> hardware. The <a href="https://www.crowdsupply.com/luxonis/megaai">megaAI</a> can run ML workloads on the on-board MyriadX VPU. Luxonis (the parent company) also ran an incredibly successful <a href="https://www.kickstarter.com/projects/opencv/opencv-ai-kit">KickStarter campaign</a> for their latest camera, <a href="https://www.kickstarter.com/projects/opencv/opencv-ai-kit">OAK</a>. I highly recommend backing the project - because this is one incredible camera.</p>
<h2 id="running-traditional-cv-algorithms">Running traditional CV Algorithms</h2>
<p>The <code class="prettyprint">DepthAI</code> cameras ( <code class="prettyprint">megaAI</code>, <code class="prettyprint">OAK</code>, <code class="prettyprint">OAK-D</code> et.al.) can run accelerated ML workloads on the dedicated VPU. The Depth AI SDK makes it easy to download new models, thanks to a Model Zoo.</p>
<p>However, i also wanted to be able to accelerate <code class="prettyprint">traditional</code> Computer Vision algorithms. Simple things like contrast stretching, thresholding, segmentation etc.</p>
<h3 id="opencl-">OpenCL ?</h3>
<p>The MyriadX VPU is an Intel project. The SDK is an fully <a href="https://github.com/openvinotoolkit/openvino">open source</a> project. </p>
<p>My first instinct was to be able to use <code class="prettyprint">OpenCL</code>, because that is exactly what Intel used <a href="https://github.com/openvinotoolkit/openvino/blob/master/inference-engine/src/vpu/custom_kernels">for their implementation</a>. </p>
<p> <code class="prettyprint">OpenCL</code> as an SDK has not aged well. Also, I am not an expert when it comes to OpenCL. The most experience I have had with an OpenCL like SDK was <a href="https://developer.android.com/guide/topics/renderscript/compute">RenderScript</a> on Android. </p>
<p>The OpenVINO <a href="(https://docs.openvinotoolkit.org/latest/index.html">documentation</a>) also did not make it any easier. First, it was unclear if <code class="prettyprint">OpenCL</code> was actually officially supported. It looks like they might have supported it on their NCS Compute Stick but there was very little documentation about MyriadX support.</p>
<p>I finally found some <a href="https://github.com/david-drew/OpenVINO-Custom-Layers/tree/master/2019.r2.0">documentation</a> which made it seem like this could work. I eventually managed to compile a super simple <code class="prettyprint">OpenCL</code> kernel but the development experience was unsatisfactory. I found it very hard to be able to debug the kernel despite it being &lt; 30 lines of code.</p>
<h3 id="ml-frameworks-for-vectorized-math">ML Frameworks for Vectorized Math</h3>
<p>I took a step back, and then I realized that I might have been approaching this problem the wrong way. </p>
<p>I realized that I could just build a custom PyTorch model that could run on the MyriadX VPU (using Intel's Model Optimizer). Only, the model was not going to a real ML model, but an entry point for me to run vectorized math on the VPU.</p>
<p>To test my idea, I wrote a very basic "model" that converted RGB images to Gray Scale images. Here is what the source looked like:</p>
<pre class="prettyprint linenums"><code class="python language-python">class Grayscale(nn.Module):
    """
    Converts a batch of RGB -&gt; Gray Scale
    1, H, W, C
    """

    def __init__(self, shape: Tuple[int, int, int, int], dtype=torch.float):
        super(Grayscale, self).__init__()
        self.shape = shape
        self.dtype = dtype

    def forward(self, x):
        # A simplified version of GrayScale.
        # 1, H, W, C as BGR
        y_b = x[0, :, :, 0]
        y_g = x[0, :, :, 1]
        y_r = x[0, :, :, 2]
        g_ = 0.3 * y_r + 0.59 * y_g + 0.11 * y_b
        # Add the channel
        g_ = torch.unsqueeze(g_, dim=2)
        # Add the batch
        g_ = torch.unsqueeze(g_, dim=0)
        return g_</code></pre>
<p>The above <code class="prettyprint">nn.Module</code> expects a batch of <code class="prettyprint">tensor</code>s. The <code class="prettyprint">batch_size</code> being used is <code class="prettyprint">1</code> (because we are only doing inference).  The image channel ordering is <code class="prettyprint">BGR</code>. </p>
<h4 id="exporting-the-model-to-onnx">Exporting the model to ONNX</h4>
<p>The OpenVINO SDK ( <code class="prettyprint">v2020.1</code>) does not officially support PyTorch so we need to export the model to an intermediate format that is supported. Fortunately this is super easy to do.</p>
<pre class="prettyprint linenums"><code class="python language-python">def export():
    output_dir = Path(__file__).parent / 'out'
    output_dir.mkdir(parents=True, exist_ok=True)
    export_onnx(output_dir=output_dir)
    print('Done.')


def export_onnx(output_dir):
    """
    Exports the model to an ONNX file.
    """
    # Define the expected input shape (dummy input)
    shape = (1, 300, 300, 3)
    # Create the Model
    model = GrayscaleModel(shape=shape, dtype=torch.float)
    X = torch.ones(shape, dtype=torch.float)
    torch.onnx.export(
        model,
        X,
        f'{output_dir.as_posix()}/model.onnx',
        opset_version=9,
        do_constant_folding=True
    )</code></pre>
<h4 id="producing-ir-using-the-openvino-model-optimizer">Producing IR using the OpenVINO Model Optimizer</h4>
<p>The Model Optimizer in OpenVINO's SDK converts an <code class="prettyprint">ONNX</code> model to an intermediate representation ( <code class="prettyprint">IR</code>). This is a way for Intel to abstract away ML frameworks being used. They support ( <code class="prettyprint">TensorFlow</code>, <code class="prettyprint">ONNX</code>, <code class="prettyprint">Caffe</code> etc.)</p>
<p>When producing the <code class="prettyprint">IR</code> we also need to provide a couple of additional hints to the optimizer. We are using a data type <code class="prettyprint">half</code> which represents <code class="prettyprint">FP16</code>, and a <code class="prettyprint">batch</code> size of <code class="prettyprint">1</code>.</p>
<pre class="prettyprint linenums"><code class="bash language-bash"># Use FP16 and make the batch_size explicit.
python mo_onnx.py --input_model "/path/to/model.onnx" --data_type half --batch 1</code></pre>
<p>This produces a list of files. </p>
<ul>
<li> <code class="prettyprint">model.xml</code> is an XML representation of the model graph (the <code class="prettyprint">IR</code>).</li>
<li> <code class="prettyprint">model.mapping</code> is a list of mappings. It's unclear what this is used for.</li>
</ul>
<h4 id="converting-ir-to-a-blob">Converting IR to a BLOB</h4>
<p>Once you have Model <code class="prettyprint">IR</code>, the final step is to convert it into a proprietary <code class="prettyprint">blob</code>.</p>
<pre class="prettyprint linenums"><code class="bash language-bash">./myriad_compile -m  path/to/model.xml -o /output/path/to/model.blob -ip U8 -VPU_MYRIAD_PLATFORM VPU_MYRIAD_2480 -VPU_NUMBER_OF_SHAVES 4 -VPU_NUMBER_OF_CMX_SLICES 4</code></pre>
<p>You should now be able to run <code class="prettyprint">model.blob</code> on the MyriadX VPU.</p>
<h4 id="running-the-model">Running the Model</h4>
<p>The DepthAI SDK makes it super easy to run the model. All you need to do is to create a <code class="prettyprint">pipeline</code> where the outputs of the <code class="prettyprint">camera</code> are linked as an input to the <code class="prettyprint">model</code>. </p>
<p>Finally the <code class="prettyprint">output</code>s of the model are connected to the <code class="prettyprint">host</code> device that the camera is connected to. </p>
<pre class="prettyprint linenums"><code class="python language-python">from pathlib import Path

import cv2
import numpy as np

import depthai as dai

if __name__ == "__main__":
    model_path = Path(__file__).parent / 'custom_ops/out'
    pipeline = dai.Pipeline()
    # Source
    camera = pipeline.createColorCamera()
    camera.setPreviewSize(300, 300)
    camera.setCamId(0)
    camera.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P)
    camera.setInterleaved(True)
    # Ops
    detection = pipeline.createNeuralNetwork()
    blob_path = model_path / 'model.blob'
    detection.setBlobPath(f'{blob_path.as_posix()}')
    # Link Camera -&gt; Model
    camera.preview.link(detection.input)
    # Link Model Output -&gt; Host
    x_out = pipeline.createXLinkOut()
    x_out.setStreamName('custom')
    detection.out.link(x_out.input)

    device = dai.Device(pipeline)
    device.startPipeline()

    frame_buffer = device.getOutputQueue(name='custom', maxSize=4)

    while True:
        frame = frame_buffer.get()
        # Model output
        layer = frame.getFirstLayerFp16()
        # Reshape
        layer = np.array(layer, dtype=np.uint8)
        shape = (300, 300, 1)
        frame_data = layer.reshape(shape)
        cv2.imshow('Image', frame_data)
        if cv2.waitKey(1) == ord('q'):
            break</code></pre>
<p>This produces something like:</p>
<p><img class="card"  src="/assets/images/depth_ai_end_to_end.png" alt="DepthAI Demo" /></p>
<p>You are looking at a <code class="prettyprint">gray scaled</code> version of my office. Even though this seems like a lot of work to get a gray scaled image, we can easily swap the model to something a lot <em>more</em> complicated, and still use rest of the pipeline.</p>
<h3 id="conclusion">Conclusion</h3>
<p>We successfully ran vectorized math on the VPU by taking advantage of the existing ML pipeline. This means, that we don't have to write any <code class="prettyprint">OpenCL</code>, and given the model is a traditional <code class="prettyprint">PyTorch</code> model, we have access to a real debugger. This makes for a much more pleasant developer experience. </p>
<h3 id="epilogue">Epilogue</h3>
<ul>
<li>I want to thank the folks at Luxnonis for answering all my questions patiently, as I was working on this approach.</li>
<li>The source code is available <a href="https://github.com/tikurahul/depthai">here</a>.</li>
<li>I am using the Depth AI <code class="prettyprint">gen2-pipeline</code>. The documentation for their new pipeline API is <a href="https://docs.luxonis.com/projects/api/en/docs_python_api/">here</a>.</li>
<li>When Installing the OpenVINO SDK, do <em>not</em> use their official scripts. The scripts install python packages <em>globally</em> rather than defaulting to a <code class="prettyprint">virtualenv</code>. This makes things a lot more complicated. Intel, if you are listenting, please make your <br />
scripts default to a virtual environment. </li>
</ul>
        <section class="footer">
          <p>
          Rahul Ravikumar &nbsp;
            <a rel="me" href="https://github.com/tikurahul">GitHub</a> &nbsp; | &nbsp;
            <a rel="me" href="https://bsky.app/profile/rahulrav.com">Bluesky</a> &nbsp; | &nbsp;
            <a rel="me" href="https://www.linkedin.com/in/rahulrav/">LinkedIn</a> &nbsp; | &nbsp;
            <a rel="me" href="https://androiddev.social/@rahulrav">AndroidDev Mastodon</a> &nbsp; | &nbsp;
            <a rel="me" href="https://rahulrav.svbtle.com/">Svbtle</a> &nbsp; | &nbsp;
            <a rel="me" href="https://twitter.com/tikurahul">Twitter</a> &nbsp;
          </p>
        </section>
      </main>
    </div>
  
        <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Roboto&family=Zilla+Slab&display=swap">
        <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=JetBrains+Mono&display=swap">
        <link rel="stylesheet" href="/assets/highlight/styles/dracula.css">
        <link rel="stylesheet" href="/assets/core/page.css">
        <script type="text/javascript" src="/assets/core/page.js"></script>
        <script type="text/javascript" src="/assets/highlight/highlight.pack.js"></script>
        <script type="text/javascript">
          // Highlight code snippets
          document.addEventListener('DOMContentLoaded', (event) => {
            document.querySelectorAll('pre code').forEach((block) => {
              hljs.highlightBlock(block);
            });
          });
        </script>
      </body>
    </html>
  